#!/bin/bash
# Copyright 2017 The Bootkube-CI Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
### Declare colors to use during the running of this script:
declare -r GREEN="\033[0;32m"
declare -r RED="\033[0;31m"
declare -r YELLOW="\033[0;33m"

function echo_green {
  echo -e "${GREEN}$1"; tput sgr0
}
function echo_red {
  echo -e "${RED}$1"; tput sgr0
}
function echo_yellow {
  echo -e "${YELLOW}$1"; tput sgr0
}

set -x

### PREPARE THE ENVIRONMENT:
source ./.osh_env
echo_green "\nAdding additional variables for an OpenStack-Helm deployment:"
printf "MARIADB_SIZE: $MARIADB_SIZE
CEPH_RGW_KEYSTONE_ENABLED: $CEPH_RGW_KEYSTONE_ENABLED
OSH_BRANCH: $OSH_BRANCH
CEPH_CLUSTER_NETWORK: $CEPH_CLUSTER_NETWORK
CEPH_PUBLIC_NETWORK: $CEPH_PUBLIC_NETWORK
GLANCE_BACKEND: $GLANCE_BACKEND \n \n"

### APPLY DEVELOPMENT RBAC POLICY:
#kubectl update -f https://raw.githubusercontent.com/openstack/openstack-helm/master/tools/kubeadm-aio/assets/opt/rbac/dev.yaml --validate=false
sudo kubectl replace -f https://raw.githubusercontent.com/openstack/openstack-helm/master/tools/kubeadm-aio/assets/opt/rbac/dev.yaml

### PREPARE DEPENDENCIES:
sudo apt-get install -y python-minimal ceph-common

### DOWNLOAD: Helm
if [[ ! -e '$TMPDIR'/'$HELM_VERSION'-helm-$KUBE_ARCH.tgz && ! -e /usr/local/bin/helm ]]; then
    wget -O $TMPDIR/$HELM_VERSION-helm-$KUBE_ARCH.tgz https://storage.googleapis.com/kubernetes-helm/helm-$HELM_VERSION-linux-$KUBE_ARCH.tar.gz
    tar zxvf $TMPDIR/$HELM_VERSION-helm-$KUBE_ARCH.tgz -C $TMPDIR/
    mv $TMPDIR/linux-$KUBE_ARCH/ $TMPDIR/$HELM_VERSION-helm-$KUBE_ARCH
    chmod +x $TMPDIR/$HELM_VERSION-helm-$KUBE_ARCH/helm
    sudo cp $TMPDIR/$HELM_VERSION-helm-$KUBE_ARCH/helm /usr/local/bin/
fi

### DOWNLOAD: OpenStack-Helm
git clone https://github.com/openstack/openstack-helm.git $BOOTKUBE_DIR/bootkube-ci/openstack-helm || true
cd $BOOTKUBE_DIR/bootkube-ci/openstack-helm && git checkout $OSH_BRANCH

### LABEL THE NODES:
kubectl label nodes openstack-control-plane=enabled --all --overwrite=true
kubectl label nodes ceph-mon=enabled --all --overwrite=true
kubectl label nodes ceph-osd=enabled --all --overwrite=true
kubectl label nodes ceph-mds=enabled --all --overwrite=true
kubectl label nodes ceph-rgw=enabled --all --overwrite=true
kubectl label nodes openvswitch=enabled --all --overwrite=true
kubectl label nodes openstack-compute-node=enabled --all --overwrite=true

### PREPARE HELM:
helm init
### WAIT FOR TILLER TO COME UP:
echo -e -n "Waiting for a ready Tiller pod..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n kube-system --no-headers 2>/dev/null | grep "Running" | grep "tiller" | wc -l)
  ### Expect Tiller pods to be equal or greater than 1:
  if [ "$running_count" -ge 1 ]; then
    break
  fi
  echo -n "."
  sleep 1
done
echo_green "SUCCESS"
echo_green "Tiller Ready!"
echo ""

### START HELM LOCAL REPO:
helm serve &
helm repo remove stable || true
helm repo add $HELM_REPO "http://localhost:8879/charts"

pwd
( cd $BOOTKUBE_DIR/bootkube-ci/openstack-helm ; make )
( cd $BOOTKUBE_DIR/bootkube-ci/openstack-helm-addons ; make )

### BRING UP CEPH STORAGE:
sudo helm install --namespace=ceph $HELM_REPO/ceph --name=ceph \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=$CEPH_RGW_KEYSTONE_ENABLED \
  --set network.public=$CEPH_PUBLIC_NETWORK \
  --set network.cluster=$CEPH_CLUSTER_NETWORK \
  --set deployment.storage_secrets=true \
  --set deployment.ceph=true \
  --set deployment.rbd_provisioner=true \
  --set deployment.client_secrets=false \
  --set deployment.rgw_keystone_user_and_endpoints=false \
  --set bootstrap.enabled=true

### RUN VARIOUS CEPH CHECKS BEFORE CONTINUING:
echo -e -n "Waiting for all Ceph Monitors to be in a running state..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n ceph --no-headers 2>/dev/null | grep "1/1" | grep "ceph-mon" | wc -l)
  ### Expect all components to be out of a "ContainerCreating" state before collecting log data (this includes CrashLoopBackOff states):
  if [ "$running_count" -ge 2 ]; then
    break
  fi
  echo -n "."
  sleep 1
done


echo -e -n "Waiting for all Ceph OSDs to be in a running state..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n ceph --no-headers 2>/dev/null | grep "1/1" | grep "ceph-osd" | wc -l)
  ### Expect all components to be out of a "ContainerCreating" state before collecting log data (this includes CrashLoopBackOff states):
  if [ "$running_count" -ge 1 ]; then
    break
  fi
  echo -n "."
  sleep 1
done


echo -e -n "Waiting for all Ceph RADOS Gateway to be in a running state..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n ceph --no-headers 2>/dev/null | grep "1/1" | grep "ceph-rgw" | wc -l)
  ### Expect all components to be out of a "ContainerCreating" state before collecting log data (this includes CrashLoopBackOff states):
  if [ "$running_count" -ge 1 ]; then
    break
  fi
  echo -n "."
  sleep 1
done

### INSTALL CEPH KEYS:
sudo helm install --namespace=openstack $HELM_REPO/ceph --name=ceph-openstack-config \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=$CEPH_RGW_KEYSTONE_ENABLED \
  --set network.public=$CEPH_PUBLIC_NETWORK \
  --set network.cluster=$CEPH_CLUSTER_NETWORK \
  --set deployment.storage_secrets=false \
  --set deployment.ceph=false \
  --set deployment.rbd_provisioner=false \
  --set deployment.client_secrets=true \
  --set deployment.rgw_keystone_user_and_endpoints=false

# NOTE: The following lines create the claims in ceph for Glance (images) and cinder (volumes).
echo -e -n "Waiting for all Ceph components to be in a running state..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n ceph --no-headers 2>/dev/null | grep "Running" | grep "ceph" | wc -l)
  ### Expect all components to be out of a "ContainerCreating" state before collecting log data (this includes CrashLoopBackOff states):
  if [ "$running_count" -ge 6 ]; then
    break
  fi
  echo -n "."
  sleep 1
done
#exit

### BRING UP THE DATABASE:
helm install --name=mariadb $HELM_REPO/mariadb --namespace=openstack \
  --set pod.replicas.server=$MARIADB_REPLICAS

echo -e -n "Waiting for mariadb components to be in a running state..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n openstack --no-headers 2>/dev/null | grep "Running" | grep "mariadb" | wc -l)
  ### Expect all components to be out of a "ContainerCreating" state before collecting log data (this includes CrashLoopBackOff states):
  if [ "$running_count" -ge $MARIADB_REPLICAS ]; then
    break
  fi
  echo -n "."
  sleep 1
done

### BRING UP THE INFRASTRUCTURE COMPONENTS:
helm install --name=memcached $HELM_REPO/memcached --namespace=openstack
helm install --name=etcd-rabbitmq $HELM_REPO/etcd --namespace=openstack
helm install --name=rabbitmq $HELM_REPO/rabbitmq --namespace=openstack \
  --set pod.replicas.server=$RABBITMQ_REPLICAS
helm install --name=ingress $HELM_REPO/ingress --namespace=openstack
helm install --name=libvirt $HELM_REPO/libvirt --namespace=openstack
helm install --name=openvswitch $HELM_REPO/openvswitch --namespace=openstack \
  --set network.interface.external=$OVS_EXTERNAL_IFACE

### BRING UP KEYSTONE:
echo "Deploying keystone with overrides:"
echo "pod:
  replicas:
    api: 2

bootstrap:
  enabled: true
  script: |
    openstack project create --description 'Charter CTEC' $OS_ADMIN_PROJECT
    openstack user create --password $OS_ADMIN_PASSWORD $OS_ADMIN_USERNAME
    openstack role add --user $OS_ADMIN_USERNAME --user-domain default --project $OS_ADMIN_PROJECT --project-domain default 'admin'
    openstack project create --description 'Demo Project' $OS_SVC_PROJECT
    openstack user create --password $OS_SVC_PASSWORD $OS_SVC_USERNAME
    openstack role add --user $OS_SVC_USERNAME --user-domain default --project $OS_ADMIN_PROJECT --project-domain default '_member_'

manifests:
  job_db_drop: true" | tee $BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/keystone_mvp.yaml
helm install --namespace=openstack --name=keystone $HELM_REPO/keystone \
   --values=$BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/keystone_mvp.yaml

### BRING UP KEYSTONE ENDPOINTS FOR CEPH:
helm install --namespace=openstack $HELM_REPO/ceph --name=radosgw-openstack \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \
  --set network.public=$CEPH_PUBLIC_NETWORK \
  --set network.cluster=$CEPH_PUBLIC_NETWORK \
  --set deployment.storage_secrets=false \
  --set deployment.ceph=false \
  --set deployment.rbd_provisioner=false \
  --set deployment.client_secrets=false \
  --set deployment.rgw_keystone_user_and_endpoints=true

### BRING UP HORIZON INTERFACE:
echo "Deploying horizon with overrides:"
echo "network:
  enable_node_port: true

manifests:
  job_db_drop: true" | tee $BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/horizon_mvp.yaml
helm install --namespace=openstack --name=horizon $HELM_REPO/horizon \
  --values=$BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/horizon_mvp.yaml

### BRING UP GLANCE:
echo "Deploying glance with overrides:"
echo "pod:
  replicas:
    api: 2
    registry: 2

endpoints:
  identity:
    auth:
      admin:
        username: $OS_ADMIN_USERNAME
        project_name: $OS_ADMIN_PROJECT
      user:
        project_name: $OS_SVC_PROJECT

manifests:
  job_drop_db: true" | tee $BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/glance_mvp.yaml
helm install --namespace=openstack --name=glance $HELM_REPO/glance \
  --values=$BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/glance_mvp.yaml \
  --set storage=$GLANCE_BACKEND

### BRING UP HEAT ORCHESTRATION FOR OPENSTACK:
echo "Deploying heat with overrides:"
echo "endpoints:
  identity:
    auth:
      admin:
        username: charter_admin
        project_name: ctec
      user:
        project_name: demo
      trustee:
        project_name: demo

manifests:
  job_db_drop: true" | tee $BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/heat_mvp.yaml
helm install --namespace=openstack --name=heat $HELM_REPO/heat \
  --values=$BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/heat_mvp.yaml

### BRING UP NOVA:
echo "Deploying nova with overrides:"
echo "conf:
  nova:
    libvirt:
      virt_type: qemu

bootstrap:
  flavors:
    options:
      vnf_tiny:
          name: "vnf.tiny"
          id: "auto"
          ram: 512
          disk: 1
          vcpus: 1
      vnf_small:
          name: "vnf.small"
          id: "auto"
          ram: 2048
          disk: 20
          vcpus: 1
      vnf_medium:
          name: "vnf.medium"
          id: "auto"
          ram: 4096
          disk: 40
          vcpus: 2
      vnf_large:
          name: "vnf.large"
          id: "auto"
          ram: 8192
          disk: 80
          vcpus: 4
      vnf_xlarge:
          name: "vnf.xlarge"
          id: "auto"
          ram: 16384
          disk: 160
          vcpus: 8

network:
  metadata:
    ip: 10.96.120.234

manifests:
  job_db_drop: true

endpoints:
  identity:
    auth:
      admin:
        username: $OS_ADMIN_USERNAME
        project_name: $OS_ADMIN_PROJECT
      user:
        project_name: $OS_SVC_PROJECT
      neutron:
        project_name: $OS_SVC_PROJECT" | tee $BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/nova_mvp.yaml
helm install --namespace=openstack $HELM_REPO/nova --name=nova \
  --values=$BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/nova_mvp.yaml

### BRING UP NEUTRON:
echo "Deploying neutron with overrides:"
echo "bootstrap:
  enabled: true
  script: |
    openstack network create --share --external --provider-physical-network "$PROVIDER_NETWORK" --provider-network-type vlan --provider-segment '$PROVIDER_SEGMENT' provider-$PROVIDER_SEGMENT

    openstack subnet create --subnet-range "$SUBNET_RANGE" --gateway "$GATEWAY" --network provider-$PROVIDER_SEGMENT --allocation-pool start="$ALLOC_POOL_START",end="$ALLOC_POOL_END" --dns-nameserver "$DNS_NAMESERVER" provider-subnet

network:
  interface:
    external: $OVS_EXTERNAL_IFACE

conf:
  neutron:
    DEFAULT:
      l3_ha: False
      min_l3_agents_per_router: 1
      max_l3_agents_per_router: 1
      l3_ha_network_type: vxlan
      dhcp_agents_per_network: 1
  plugins:
    ml2_conf:
      ml2_type_vlan:
        network_vlan_ranges: $PROVIDER_NETWORK:$PROVIDER_SEGMENT:$PROVIDER_SEGMENT
    openvswitch_agent:
      agent:
        tunnel_types: vxlan
      ovs:
        bridge_mappings: public:br-ex
  metadata_agent:
    DEFAULT:
      nova_metadata_ip: 10.96.120.234

endpoints:
  identity:
    auth:
      admin:
        username: $OS_ADMIN_USERNAME
        project_name: $OS_ADMIN_PROJECT
      user:
        project_name: $OS_SVC_PROJECT
      nova:
        project_name: $OS_SVC_PROJECT

manifests:
  job_db_drop: true" | tee $BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/neutron_mvp.yaml
helm install --namespace=openstack $HELM_REPO/neutron --name=neutron \
  --values=$BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/neutron_mvp.yaml

### BRING UP CINDER:
echo "Deploying cinder with overrides:"
echo "pod:
  replicas:
    api: 2

endpoints:
  identity:
    auth:
      admin:
        username: $OS_ADMIN_USERNAME
        project_name: $OS_ADMIN_PROJECT
      user:
        project_name: $OS_SVC_PROJECT

manifests:
  job_drop_db: true" | tee $BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/cinder_mvp.yaml
helm install --namespace=openstack --name=cinder $HELM_REPO/cinder \
  --values=$BOOTKUBE_DIR/bootkube-ci/deploy-addons/demo/cinder_mvp.yaml
