#!/bin/bash
# Copyright 2017 The Bootkube-CI Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
### Declare colors to use during the running of this script:
declare -r GREEN="\033[0;32m"
declare -r RED="\033[0;31m"
declare -r YELLOW="\033[0;33m"

function echo_green {
  echo -e "${GREEN}$1"; tput sgr0
}
function echo_red {
  echo -e "${RED}$1"; tput sgr0
}
function echo_yellow {
  echo -e "${YELLOW}$1"; tput sgr0
}

### PREPARE THE ENVIRONMENT:
source ./.osh_env
echo_green "\nAdding additional variables for an OpenStack-Helm deployment:"
printf "MARIADB_SIZE: $MARIADB_SIZE
CEPH_RGW_KEYSTONE_ENABLED: $CEPH_RGW_KEYSTONE_ENABLED
OSH_BRANCH: $OSH_BRANCH
CEPH_CLUSTER_NETWORK: $CEPH_CLUSTER_NETWORK
CEPH_PUBLIC_NETWORK: $CEPH_PUBLIC_NETWORK
GLANCE_BACKEND: $GLANCE_BACKEND \n \n"

### APPLY DEVELOPMENT RBAC POLICY:
#kubectl update -f https://raw.githubusercontent.com/openstack/openstack-helm/master/tools/kubeadm-aio/assets/opt/rbac/dev.yaml --validate=false
sudo kubectl replace -f /opt/demo/deploy-rbac/demo.yaml

### PREPARE DEPENDENCIES:
sudo apt-get install -y python-minimal ceph-common

### DOWNLOAD: OpenStack-Helm
git clone https://github.com/openstack/openstack-helm.git $BOOTKUBE_DIR/bootkube-ci/openstack-helm && cd $BOOTKUBE_DIR/bootkube-ci/openstack-helm && git checkout $OSH_BRANCH

### LABEL THE NODES:
kubectl label nodes openstack-control-plane=enabled --all --overwrite=true
kubectl label nodes ceph-mon=enabled --all --overwrite=true
kubectl label nodes ceph-osd=enabled --all --overwrite=true
kubectl label nodes ceph-mds=enabled --all --overwrite=true
kubectl label nodes ceph-rgw=enabled --all --overwrite=true
kubectl label nodes openvswitch=enabled --all --overwrite=true
kubectl label nodes openstack-compute-node=enabled --all --overwrite=true

### PREPARE HELM:
helm init
### WAIT FOR TILLER TO COME UP:
echo -e -n "Waiting for a ready Tiller pod..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n kube-system --no-headers 2>/dev/null | grep "Running" | grep "tiller" | wc -l)
  ### Expect Tiller pods to be equal or greater than 1:
  if [ "$running_count" -ge 1 ]; then
    break
  fi
  echo -n "."
  sleep 1
done
echo_green "SUCCESS"
echo_green "Tiller Ready!"
echo ""

### START HELM LOCAL REPO:
helm serve &
helm repo remove stable
helm repo add $HELM_REPO "http://localhost:8879/charts"
#sudo mkdir -p /var/lib/nova/instances
pwd
make

### BRING UP CEPH STORAGE:
sudo helm install --namespace=ceph $HELM_REPO/ceph --name=ceph \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=$CEPH_RGW_KEYSTONE_ENABLED \
  --set network.public=$CEPH_PUBLIC_NETWORK \
  --set network.cluster=$CEPH_CLUSTER_NETWORK \
  --set deployment.storage_secrets=true \
  --set deployment.ceph=true \
  --set deployment.rbd_provisioner=true \
  --set deployment.client_secrets=false \
  --set deployment.rgw_keystone_user_and_endpoints=false \
  --set bootstrap.enabled=true

### RUN VARIOUS CEPH CHECKS BEFORE CONTINUING:
echo -e -n "Waiting for all Ceph Monitors to be in a running state..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n ceph --no-headers 2>/dev/null | grep "1/1" | grep "ceph-mon" | wc -l)
  ### Expect all components to be out of a "ContainerCreating" state before collecting log data (this includes CrashLoopBackOff states):
  if [ "$running_count" -ge 2 ]; then
    break
  fi
  echo -n "."
  sleep 1
done


echo -e -n "Waiting for all Ceph OSDs to be in a running state..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n ceph --no-headers 2>/dev/null | grep "1/1" | grep "ceph-osd" | wc -l)
  ### Expect all components to be out of a "ContainerCreating" state before collecting log data (this includes CrashLoopBackOff states):
  if [ "$running_count" -ge 1 ]; then
    break
  fi
  echo -n "."
  sleep 1
done


echo -e -n "Waiting for all Ceph RADOS Gateway to be in a running state..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n ceph --no-headers 2>/dev/null | grep "1/1" | grep "ceph-rgw" | wc -l)
  ### Expect all components to be out of a "ContainerCreating" state before collecting log data (this includes CrashLoopBackOff states):
  if [ "$running_count" -ge 1 ]; then
    break
  fi
  echo -n "."
  sleep 1
done

### INSTALL CEPH KEYS:
sudo helm install --namespace=openstack $HELM_REPO/ceph --name=ceph-openstack-config \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=$CEPH_RGW_KEYSTONE_ENABLED \
  --set network.public=$CEPH_PUBLIC_NETWORK \
  --set network.cluster=$CEPH_CLUSTER_NETWORK \
  --set deployment.storage_secrets=false \
  --set deployment.ceph=false \
  --set deployment.rbd_provisioner=false \
  --set deployment.client_secrets=true \
  --set deployment.rgw_keystone_user_and_endpoints=false

# NOTE: The following lines create the claims in ceph for Glance (images) and cinder (volumes).
echo -e -n "Waiting for all Ceph components to be in a running state..."
while true; do
  running_count=$(sudo kubectl --kubeconfig=/etc/kubernetes/kubeconfig get pods -n ceph --no-headers 2>/dev/null | grep "Running" | grep "ceph" | wc -l)
  ### Expect all components to be out of a "ContainerCreating" state before collecting log data (this includes CrashLoopBackOff states):
  if [ "$running_count" -ge 6 ]; then
    break
  fi
  echo -n "."
  sleep 1
done
exit

### BRING UP THE DATABASE:
helm install --name=mariadb $HELM_REPO/mariadb --namespace=openstack

### BRING UP THE INFRASTRUCTURE COMPONENTS:
helm install --name=memcached $HELM_REPO/memcached --namespace=openstack
helm install --name=etcd-rabbitmq $HELM_REPO/etcd --namespace=openstack
helm install --name=rabbitmq $HELM_REPO/rabbitmq --namespace=openstack
helm install --name=ingress $HELM_REPO/ingress --namespace=openstack
helm install --name=libvirt $HELM_REPO/libvirt --namespace=openstack
helm install --name=openvswitch $HELM_REPO/openvswitch --namespace=openstack

### BRING UP KEYSTONE:
helm install --namespace=openstack --name=keystone $HELM_REPO/keystone \
  --set pod.replicas.api=2

### BRING UP KEYSTONE ENDPOINTS FOR CEPH:
helm install --namespace=openstack $HELM_REPO/ceph --name=radosgw-openstack \
  --set endpoints.identity.namespace=openstack \
  --set endpoints.object_store.namespace=ceph \
  --set endpoints.ceph_mon.namespace=ceph \
  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \
  --set network.public=$CEPH_PUBLIC_NETWORK \
  --set network.cluster=$CEPH_PUBLIC_NETWORK \
  --set deployment.storage_secrets=false \
  --set deployment.ceph=false \
  --set deployment.rbd_provisioner=false \
  --set deployment.client_secrets=false \
  --set deployment.rgw_keystone_user_and_endpoints=true

### BRING UP HORIZON INTERFACE:
helm install --namespace=openstack --name=horizon $HELM_REPO/horizon \
  --set network.enable_node_port=true

### BRING UP GLANCE:
helm install --namespace=openstack --name=glance $HELM_REPO/glance \
  --set pod.replicas.api=2 \
  --set pod.replicas.registry=2
  --set storage=$GLANCE_BACKEND

### BRING UP HEAT ORCHESTRATION FOR OPENSTACK:
helm install --namespace=openstack --name=heat $HELM_REPO/heat

### BRING UP NEUTRON:
helm install --namespace=openstack --name=neutron $HELM_REPO/neutron \
  --set pod.replicas.server=2

### BRING UP NOVA:
helm install --namespace=openstack --name=nova $HELM_REPO/nova \
  --set pod.replicas.api_metadata=2 \
  --set pod.replicas.osapi=2 \
  --set pod.replicas.conductor=2 \
  --set pod.replicas.consoleauth=2 \
  --set pod.replicas.scheduler=2 \
  --set pod.replicas.novncproxy=2


### BRING UP CINDER:
helm install --namespace=openstack --name=cinder $HELM_REPO/cinder \
  --set pod.replicas.api=2

# Remove the exit statement if you wish any of the following Charts
Exit
helm install --name=barbican local/barbican --namespace=openstack
helm install --name=mistral local/mistral --namespace=openstack
helm install --name=magnum local/magnum --namespace=openstack
